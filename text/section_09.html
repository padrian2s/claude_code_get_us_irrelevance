<div class="page-content">
  <h2>Section 9: Military Risks & the Cold War Parallel</h2>

  <div class="transcript">
    <span class="speaker speaker--host">Ross:</span>
    <p>You say: "A swarm of billions of fully automated armed drones, locally controlled by powerful A.I., strategically coordinated across the world by even more powerful A.I., could be an unbeatable army." There's really only two countries doing intense A.I. work — the US and China. I feel like you are strongly weighted towards a future where we're staying ahead of the Chinese. But isn't it just more likely that if humanity survives, it will be because the US and Beijing are constantly sitting down, hammering out A.I. control deals?</p>

    <span class="speaker speaker--guest">Dario:</span>
    <p>I think if we end up in that world, that is actually exactly what we should do. I'm in favor of trying to work out restraints. Taking some of the worst applications of the technology — these drones, terrifying biological weapons — there is some precedent for the worst abuses being curbed, often because they're horrifying while at the same time they provide limited strategic advantage.</p>

    <p>I'm at the same time a little concerned and skeptical that when things directly provide as much power as possible, it's hard to get out of the game given what's at stake. If we go back to the Cold War, we were able to reduce the number of missiles both sides had, but we were not able to entirely forsake nuclear weapons.</p>

    <span class="speaker speaker--host">Ross:</span>
    <p>Is your skepticism rooted in the fact that AI would provide a kind of advantage that nukes did not? In the Cold War, both sides would be wiped out. You think with AI, if you got an edge, you would just win?</p>

    <span class="speaker speaker--guest">Dario:</span>
    <p>Let's look at the biological Weapons Convention. Biological weapons — they're horrifying, everyone hates them. We were able to sign the convention. The US genuinely stopped developing them. But biological weapons provide some advantage — it's not like they're the difference between winning and losing. So we were able to give them up.</p>

    <p>Having 12,000 nuclear weapons versus 5,000 — again, you can kill more people with more. But we were able to be reasonable and reduce them. But completely disarming nuclear — I don't think we ever got to that. And I think that's very hard unless you had really reliable verification. I would guess we'll end up in the same world with A.I.</p>
  </div>

  <div class="commentary">
    <h3>Explained Simply</h3>

    <div class="commentary-section">
      <h4>In Simple Terms</h4>
      <p>Dario is comparing AI to nuclear weapons and biological weapons to figure out what kind of arms control might work. His conclusion: we can probably ban the most horrifying uses of AI (like bio-weapons), and maybe reduce the scariest applications (like autonomous drone armies). But we probably CAN'T get countries to stop developing powerful AI altogether, for the same reason no one has ever fully given up nuclear weapons — the stakes are just too high.</p>
    </div>

    <div class="highlight-box">
      <strong>Dario's framework for what can be controlled:</strong><br>
      <strong>Easy to ban</strong> (like bio-weapons): Horrifying to everyone, limited strategic advantage. Both sides want to ban it.<br>
      <strong>Possible to reduce</strong> (like nuclear stockpiles): Massive advantage, but you can agree to have "less" without giving it all up.<br>
      <strong>Impossible to give up</strong> (like AI itself): The difference between having it and not is the difference between being a superpower and not. No country will voluntarily fall behind.
    </div>

    <div class="commentary-section">
      <h4>Why AI Is Different from Nukes</h4>
      <p>Nuclear weapons created "mutually assured destruction" — if you use them, you die too. That actually kept the peace in a twisted way. AI doesn't have that dynamic. If one country achieves massively superior AI, they could potentially dominate without destroying themselves. That makes the competition more dangerous, because there's no natural brake on the arms race.</p>
    </div>

    <div class="warning-box">
      <strong>The drone swarm scenario:</strong> Billions of AI-controlled armed drones, acting as a coordinated army. No human soldiers at risk on the attacking side. This is already being developed, and it fundamentally changes warfare because it removes the biggest cost of war — your own people dying. That makes war easier to start.
    </div>

    <div class="commentary-section">
      <h4>Why This Matters to You</h4>
      <p>This isn't science fiction. Autonomous weapons are already being tested. The question isn't whether AI will be used in warfare — it will. The question is whether there will be any limits. Dario thinks some limits are possible (banning bio-weapons, restricting autonomous weapons) but total restraint is unrealistic. We'll likely end up in an AI Cold War, with ongoing negotiations about what's allowed.</p>
    </div>
  </div>
</div>
