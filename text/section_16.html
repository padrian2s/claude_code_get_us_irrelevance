<div class="page-content">
  <h2>Section 16: The Big Blob of Compute — Why AI Keeps Getting Smarter</h2>

  <div class="transcript">
    <span class="speaker speaker--note">From Dario Amodei's interview on the Dwarkesh Podcast (2025) — a technically detailed, two-hour conversation about the mechanics of AI progress, concrete timelines, and the economics of the race to build a "country of geniuses in a data center." Sections 16–25 cover this interview.</span>

    <span class="speaker speaker--host">Dwarkesh:</span>
    <p>We talked three years ago. In your view, what has been the biggest update over the last three years?</p>

    <span class="speaker speaker--guest">Dario:</span>
    <p>Broadly speaking, the exponential of the underlying technology has gone about as I expected it to go. The frontier is a little bit uneven, but it's roughly what I expected in terms of the march of the models from smart high school student to smart college student to beginning to do PhD and professional stuff, and in the case of code reaching beyond that. What has been the most surprising thing is the lack of public recognition of how close we are to the end of the exponential. To me, it is absolutely wild that you have people — within the bubble and outside the bubble — talking about the same tired, old hot-button political issues, when we are near the end of the exponential.</p>

    <span class="speaker speaker--host">Dwarkesh:</span>
    <p>What is the scaling hypothesis at this point?</p>

    <span class="speaker speaker--guest">Dario:</span>
    <p>I actually have the same hypothesis I had even all the way back in 2017. I wrote a doc called "The Big Blob of Compute Hypothesis". It wasn't about the scaling of language models in particular. When I wrote it GPT-1 had just come out. That was one among many things. Back in those days there was robotics. People tried to work on reasoning as a separate thing from language models, and there was scaling of the kind of RL that happened in AlphaGo and in Dota at OpenAI.</p>

    <p>What it says is that all the cleverness, all the techniques, all the "we need a new method to do something", that doesn't matter very much. There are only a few things that matter. One is how much raw compute you have. The second is the quantity of data. The third is the quality and distribution of data — it needs to be a broad distribution. The fourth is how long you train for. The fifth is that you need an objective function that can scale to the moon. The pre-training objective function is one such objective function. Another is the RL objective function that says you have a goal, you're going to go out and reach the goal. Then the sixth and seventh were things around normalization or conditioning, just getting the numerical stability so that the big blob of compute flows in this laminar way instead of running into problems.</p>

    <p>That was the hypothesis, and it's a hypothesis I still hold. I don't think I've seen very much that is not in line with it.</p>

    <span class="speaker speaker--guest">Dario (on RL scaling):</span>
    <p>We're seeing the same scaling in RL that we saw for pre-training. Even other companies have published things that say, "We train the model on math contests — AIME or other things — and how well the model does is log-linear in how long we've trained it." We see that as well, and it's not just math contests. It's a wide variety of RL tasks.</p>
  </div>

  <div class="commentary">
    <h3>Explained Simply</h3>

    <div class="commentary-section">
      <h4>What's Happening Here</h4>
      <p>This is the opening of a long, detailed interview on the Dwarkesh Podcast. Where the Ross Douthat interview (Sections 1–14) covered the big-picture promise and peril of AI, this conversation goes deep into the machinery — why AI keeps improving, exactly how fast, and the economics driving it all. Think of Sections 1–15 as "what AI could do to the world" and Sections 16–25 as "the engine room: how the technology actually works and where it's heading."</p>
    </div>

    <div class="definition-box">
      <strong>The Big Blob of Compute Hypothesis:</strong> Dario's core belief since 2017 — AI progress isn't driven by clever breakthroughs or new techniques. It's driven by seven mundane factors: (1) raw compute, (2) data quantity, (3) data quality and breadth, (4) training duration, (5) a good scoring function, (6-7) numerical stability tricks. That's it. Every "breakthrough" people celebrate is really just one of these levers being pushed further. This is why AI progress has been so steady and predictable to insiders — it's fundamentally an engineering problem, not a series of lucky discoveries.
    </div>

    <div class="commentary-section">
      <h4>Two Phases of Training</h4>
      <p>AI models now go through two big learning phases. <strong>Pre-training</strong> is when the model reads trillions of words from the internet — every textbook, every forum post, every Wikipedia article it can find. This gives it broad knowledge. <strong>RL (Reinforcement Learning)</strong> comes after: the model practices specific tasks — solving math problems, writing code, reasoning through logic — and gets scored on how well it does. Think of pre-training as reading every book in a library, and RL as then doing thousands of practice exams. What Dario is saying here is that BOTH phases show the same pattern: spend more compute, get predictably better results. The exponential hasn't stopped.</p>
    </div>

    <div class="highlight-box">
      <strong>Log-linear scaling — what it means:</strong> When Dario says progress is "log-linear in how long we've trained it," he means that doubling your training compute gives you a fixed, predictable improvement — not double the performance, but a reliable step up. Like how doubling study hours doesn't double your test score, but it does reliably raise it by a few points. The key insight: this pattern has held for YEARS across many different tasks. It's not slowing down. It's not speeding up. It's a smooth, relentless escalator.
    </div>

    <div class="commentary-section">
      <h4>Why This Matters</h4>
      <p>If Dario is right that AI progress is mostly about scale — more compute, more data, better scoring — then progress is almost guaranteed as long as companies keep investing. There's no mystery ingredient missing. There's no wall we're about to hit. You just need more of the same things, and the resources to buy them. This is simultaneously the most optimistic and most unsettling thing about the current moment: the trajectory is predictable, and it points straight up.</p>
    </div>

    <div class="quote-box">
      <strong>"It is absolutely wild that you have people — within the bubble and outside the bubble — talking about the same tired, old hot-button political issues, when we are near the end of the exponential."</strong><br><br>
      This is Dario saying the quiet part loud. He believes we are within a few years of building something that will transform everything — the economy, medicine, warfare, governance — and almost nobody outside the AI industry is acting like it.
    </div>
  </div>
</div>
