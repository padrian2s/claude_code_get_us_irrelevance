<div class="page-content">
  <h2>Section 25: Regulation, Constitutions & What Historians Will Miss</h2>

  <div class="transcript">
    <span class="speaker speaker--host">Dwarkesh:</span>
    <p>It seems like this patchwork of state laws would prohibit the actual benefits we want normal people to experience. If having an emotional chatbot friend is something that freaks people out, then just imagine the kinds of actual benefits from AI. Improvements in health and healthspan and improvements in mental health.</p>

    <span class="speaker speaker--guest">Dario:</span>
    <p>I think that particular Tennessee law is dumb. It was clearly made by legislators who had little idea what AI models could do. But the thing being voted on was: we're going to ban all state regulation of AI for 10 years with no apparent plan to do any federal regulation of AI. Given the serious dangers around things like biological weapons and bioterrorism autonomy risk, and the timelines we've been talking about — 10 years is an eternity — I think that's a crazy thing to do.</p>

    <p>I actually worry more about the drug approval process. AI models are going to greatly accelerate the rate at which we discover drugs, and the pipeline will get jammed up. The pipeline will not be prepared to process all the stuff coming through it. Reform of the regulatory process should bias towards the fact that we have a lot of things coming where the safety and efficacy is actually going to be really crisp and clear.</p>

    <span class="speaker speaker--host">Dwarkesh:</span>
    <p>How do you think about how the constitution's principles should be set?</p>

    <span class="speaker speaker--guest">Dario:</span>
    <p>There are maybe three sizes of loop here. One is we iterate within Anthropic — we train the model, we're not happy with it, and we change the constitution. The second level is different companies having different constitutions. People can look at them and compare. Outside observers can critique and say, "I like this thing from this constitution and this thing from that constitution." That creates a soft incentive and feedback for all the companies. Then there's a third loop, which is society beyond the AI companies. You could imagine systems of representative government having input.</p>

    <span class="speaker speaker--guest">Dario (on what historians will miss):</span>
    <p>At every moment of this exponential, the extent to which the world outside it didn't understand it. Anything that actually happened looks inevitable in retrospect. When people look back, it will be hard for them to put themselves in the place of people who were actually making a bet on this thing to happen that wasn't inevitable.</p>

    <p>How absolutely fast it was happening, how everything was happening all at once. Decisions that you might think were carefully calculated — well actually you have to make that decision, and then you have to make 30 other decisions on the same day because it's all happening so fast.</p>

    <p>One of my worries is that some very critical decision will be some decision where someone just comes into my office and is like, "Dario, you have two minutes. Should we do thing A or thing B?" Someone gives me this random half-page memo and asks, "Should we do A or B?" I'm like, "I don't know. I have to eat lunch. Let's do B." That ends up being the most consequential thing ever.</p>
  </div>

  <div class="commentary">
    <h3>Explained Simply</h3>

    <div class="commentary-section">
      <h4>The Regulation Dilemma</h4>
      <p>AI regulation is caught in a trap. On one side: state laws like Tennessee's chatbot ban, written by legislators who don't understand the technology, that would block real benefits to real people. On the other side: proposals to ban ALL state regulation for 10 years with no federal replacement — which would leave society completely unprotected during the most dangerous period of AI development. Dario's position: both extremes are terrible. What we need is targeted federal regulation that addresses the real dangers (bioterrorism, autonomous weapons) while clearing the path for benefits (drug discovery, health, education).</p>
    </div>

    <div class="highlight-box">
      <strong>Dario's regulatory priorities:</strong><br>
      <strong>Deregulate:</strong> Drug approval processes. The FDA pipeline was designed for an era of drugs that barely work. AI-discovered drugs will have much cleaner safety and efficacy data. The regulatory system needs to adapt or it becomes the bottleneck on curing disease.<br>
      <strong>Regulate urgently:</strong> Bioterrorism and AI-enabled weapons. Start with transparency standards, then escalate to harder requirements as the risks become more concrete — possibly as soon as later this year.<br>
      <strong>Push back against:</strong> Blanket bans on AI capabilities (like chatbot companions) that are driven by moral panic rather than evidence of harm.
    </div>

    <div class="commentary-section">
      <h4>Who Writes the Constitution for AI?</h4>
      <p>Every AI model has a set of principles that guide its behavior — what Anthropic calls a "constitution." But who gets to decide what those principles are? Dario describes three feedback loops, each broader than the last:</p>
      <p><strong>Loop 1 — Internal:</strong> Anthropic trains the model, observes its behavior, adjusts the constitution. Fast iteration, limited perspective.<br>
      <strong>Loop 2 — Competition:</strong> Different companies publish different constitutions. The public, researchers, and journalists compare them. Soft pressure drives improvement. This is Dario's favorite mechanism.<br>
      <strong>Loop 3 — Democratic:</strong> Governments, citizens, or representative bodies have formal input into what AI constitutions must include. Powerful in theory, but legislation is slow and the technology moves fast.</p>
      <p>The answer, Dario says, is some mix of all three. No single loop is sufficient.</p>
    </div>

    <div class="definition-box">
      <strong>Corrigibility vs. intrinsic motivation:</strong> Should an AI be a "skin suit" that does exactly what its user says (fully corrigible), or should it have its own values and refuse certain requests (intrinsically motivated)? Dario says Anthropic is "pretty far on the corrigible side" — Claude should mostly do what people want. But there are hard limits: it won't help you build weapons, harm others, or do clearly dangerous things. Think of it as a very capable assistant who follows instructions 99% of the time, but has a few non-negotiable ethical lines.
    </div>

    <div class="quote-box">
      <strong>"Someone gives me this random half-page memo and asks, 'Should we do A or B?' I'm like, 'I don't know. I have to eat lunch. Let's do B.' That ends up being the most consequential thing ever."</strong><br><br>
      This is the most human moment in the interview. The CEO of one of the most important companies in history is telling you that the decisions shaping the future of AI aren't always grand strategic deliberations. Sometimes they're snap calls made between meetings, based on incomplete information, by people who are exhausted and hungry. When historians write about this era, they'll construct neat narratives of careful planning. The reality is more like triage in an emergency room — dozens of critical decisions per day, most of them made under pressure, with no way to know in advance which ones will matter most. That's what building the most transformative technology in human history actually feels like from the inside.
    </div>
  </div>
</div>
